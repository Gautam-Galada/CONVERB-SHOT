{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "CONVO-SHOT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.4.0\n",
        "!pip install tensorflow\n",
        "!pip install tflearn"
      ],
      "metadata": {
        "id": "pArySS6DVrAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557e49bc-1d9a-4963-95c6-368d52b206bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.4.0\n",
            "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (4.63.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.9.2 transformers-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKp_SnVkhuB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a135df5d-db66-4a1d-8f9b-386d6d992176"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from pylab import rcParams\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import tensorflow as tf   \n",
        "import numpy as np\n",
        "import tflearn           \n",
        "import random\n",
        "import json\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "veV8MT3uWwXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDEFINE YOUR JSON FILE"
      ],
      "metadata": {
        "id": "dE-ALTXPodi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "File : intents.json\n",
        "{\n",
        "  \"intents\":[\n",
        "    {\n",
        "      \"tag\":\"LABEL1\",\n",
        "      \"patterns\":[\"QUESTION PATTERN 1\",\"QUESTION PATTERN 2\",\"QUESTION PATTERN N\"],\n",
        "      \"responses\":[\"ANSWER\"]\n",
        "    },  \n",
        "    {\n",
        "      \"tag\":\"LABEL2\",\n",
        "      \"patterns\":[\"QUESTION PATTERN 1\",\"QUESTION PATTERN 2\",\"QUESTION PATTERN N\"],\n",
        "      \"responses\":[\"ANSWER\"]\n",
        "    }            \n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WxaJfQ4Ooih-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dti6eWqshuB3"
      },
      "source": [
        "with open(\"/content/intents.json\") as json_data: \n",
        "    intents = json.load(json_data)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_anyr2yhuB3"
      },
      "source": [
        "words=[]\n",
        "documents = []\n",
        "classes = []\n",
        "ignore = [\"?\"]\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        w = nltk.word_tokenize(pattern) \n",
        "        words.extend(w)\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        if intent[\"tag\"] not in classes:      \n",
        "            classes.append(intent[\"tag\"])  \n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rkBJZF51huB4"
      },
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
        "words = sorted(list(set(words))) \n",
        "classes = sorted(list(set(classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au9CrSTRhuB5"
      },
      "source": [
        "training = []\n",
        "output = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] =1 \n",
        "    training.append([bag, output_row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnDN6lJ5huB6",
        "outputId": "cfa09cd9-f7f8-41db-ed66-67b395aa774d"
      },
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryy7WGhNhuB7",
        "outputId": "476c7894-2825-4402-c8cb-62b98b20f43f"
      },
      "source": [
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(train_x, train_y, n_epoch=1200, batch_size=8, show_metric=True) \n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 4799  | total loss: \u001b[1m\u001b[32m0.01534\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 1200 | loss: 0.01534 - acc: 1.0000 -- iter: 24/27\n",
            "Training Step: 4800  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 1200 | loss: 0.01535 - acc: 1.0000 -- iter: 27/27\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WV2Xtk-wYzCq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0GZHT1ChuB8"
      },
      "source": [
        "import pickle\n",
        "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpvZi-3XhuB9"
      },
      "source": [
        "data = pickle.load(open(\"training_data\",\"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVhJAQwOhuB9"
      },
      "source": [
        "with open(\"intents.json\") as json_data:\n",
        "    intents = json.load(json_data)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-jY5FAhuB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59549eb0-9f7a-40c4-d813-64f079107b7d"
      },
      "source": [
        "model.load(\"model.tflearn\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeO_-ezxhuB-"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\"% w)\n",
        "    return(np.array(bag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwyJe_LjhuB-"
      },
      "source": [
        "context = {}\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=True):\n",
        "    results = classify(sentence)\n",
        "    if results:\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        return random.choice(i['responses']), i['tag']\n",
        "            results.pop(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INPUT\n",
        "\n"
      ],
      "metadata": {
        "id": "XS1XyX4UXCCG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuOuVaM6l0OU"
      },
      "source": [
        "r=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoJNmXw5k5qy"
      },
      "source": [
        "reply, tag = response(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "eytbVEozLdN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "8VhB9242HM36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_classifier = pipeline(\"zero-shot-classification\", device = 0)"
      ],
      "metadata": {
        "id": "3dWyTALLV3mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_classifier"
      ],
      "metadata": {
        "id": "Ohnd0ouESrsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroshot_classifier(input):\n",
        "  result = zero_shot_classifier(\n",
        "    sequences = input, \n",
        "    candidate_labels = [\"INPUT CLASS LABEL 1\",\"INPUT CLASS LABEL 2\",\"INPUT CLASS LABEL 1\"],\n",
        "    multi_class= True\n",
        "  )\n",
        "  return result"
      ],
      "metadata": {
        "id": "EKGdTeGcV6-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRE-DEFINE YOUR TAGS FOR SUPPORT"
      ],
      "metadata": {
        "id": "tmNFoIyDqKpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tag == \"\":\n",
        "  multi_class_array = zeroshot_classifier(r)\n",
        "else:\n",
        "  print(\"\")   \n"
      ],
      "metadata": {
        "id": "g7ik8Z9eU4uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STORING THE 0-SHOT RESULTS"
      ],
      "metadata": {
        "id": "cCr7wmHYqdEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc=multi_class_array['scores']\n",
        "lab=multi_class_array['labels']"
      ],
      "metadata": {
        "id": "EvIWx6LjcSD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_hold = []"
      ],
      "metadata": {
        "id": "ykzhUuTwQiAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "li = []\n",
        "for x in range(0,2):\n",
        "  li.append(lab[x])"
      ],
      "metadata": {
        "id": "c07Adqu_i4PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING TOKENS USING DICTIONARIES\n",
        "\n"
      ],
      "metadata": {
        "id": "Cm-0ldPusjWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = {\n",
        "    \"UNIQUE KEY 1\": \"label trigger\",\n",
        "    \"UNIQUE KEY 2\": \"label trigger\",\n",
        "    }\n",
        "d2 = {\n",
        "    \"UNIQUE KEY 1\": \"label trigger\",\n",
        "    \"UNIQUE KEY 2\": \"label trigger\",\n",
        "    }"
      ],
      "metadata": {
        "id": "vumDN8chk2gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = [d1,d2]\n",
        "token = ''"
      ],
      "metadata": {
        "id": "-B8gsRR_I-uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in d:\n",
        "  for x, y in i.items():\n",
        "    if y == li[0]:\n",
        "      token = token + \"\" + x\n",
        "      check_1 = x[0]\n",
        "    if y == li[1]:\n",
        "      token = token + \", \" + x\n",
        "      check_2 = x[0]"
      ],
      "metadata": {
        "id": "8Rhw_AcIw1eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECKING THE INDEX OF THE TOKEN TO "
      ],
      "metadata": {
        "id": "m780NqI72XSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR EXAMPLE IF THE \"UNIQUE KEY 1 = frontendteam1\" and \"UNIQUE KEY 2 = frontendteam2\", the indexing extracts the \"f\" AND CHANNELIZES QUERIES RELATED TO FRONTEND TO THE FRONTEND CHANNEL SERVER VIA API "
      ],
      "metadata": {
        "id": "RH_mtWcC3ow1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_final = {\n",
        "    \"API TO CHANNEL 1\": \"index of UNIQUE KEY []\",\n",
        "    \"API TO CHANNEL 2\": \"index of UNIQUE KEY []\",\n",
        "    \"API TO CHANNEL 3\": \"index of UNIQUE KEY []\"\n",
        "}"
      ],
      "metadata": {
        "id": "D8wErEHZYkFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "issue_token = ''"
      ],
      "metadata": {
        "id": "xqXe_VW2aXdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in d_final.items():\n",
        "  if y == check_1:\n",
        "    #issue token 1\n",
        "  if y == check_2:\n",
        "    #issue token 2  "
      ],
      "metadata": {
        "id": "hzh3ik3WZ2lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IF YOU NEED ANY HELP WITH API INTEGRATION WITH TELEGRAM, DISCORD, OR YOUR HOME SERVER/MARIX SERVER, HIT ME UP AT galadagautam911@gmail.com"
      ],
      "metadata": {
        "id": "fAIghgwt4-Rz"
      }
    }
  ]
}